{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Marisa Lytle, Penn State University\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Program for the automated labeling of video files, to be compared with manual labeling by hand-coding users"
      ],
      "metadata": {
        "id": "mKg-Z0D6cJ8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Necessary Packages"
      ],
      "metadata": {
        "id": "pX4L1_jxWray"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "_ZJy8aUjVExK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "# For Image Displaying:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N_-Q-YmSVFNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1099ae0-8854-4b63-a774-8740b2200d42"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective (for now) is to display an arbitrary frame to users viewing the tutorial, with a face mask. To do this we will need sample data, and an initial frame (preferably with an object like a person in view)"
      ],
      "metadata": {
        "id": "SXnNcFAokXrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Image Display\n",
        "imagepath = '/content/drive/My Drive/Colab Notebooks/images.png'\n",
        "image = cv2.imread(imagepath)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6fNCAKOQgim2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process our raw MET files dowloaded from Pupil cloud\n",
        "\n",
        "def proc_met_files(subject):\n",
        "    # Define folder path\n",
        "    folder = os.path.join(\n",
        "        '[raw data directory]',\n",
        "        subject\n",
        "    )\n",
        "\n",
        "    # Find files\n",
        "    gaze_file = next(glob.iglob(f\"{folder}/**/gaze.csv\"), None)\n",
        "    timestamps_file = next(glob.iglob(f\"{folder}/**/world_timestamps.csv\"), None)\n",
        "    events_file = next(glob.iglob(f\"{folder}/**/events.csv\"), None)\n",
        "\n",
        "    # Read data\n",
        "    events = pd.read_csv(events_file)\n",
        "    gaze = pd.read_csv(gaze_file)\n",
        "    timestamps = pd.read_csv(timestamps_file)\n",
        "\n",
        "    # Turn events (markers identified by researcher and indicated in pupil cloud) into a dictionary\n",
        "    events = events.rename(columns={\"timestamp [ns]\": \"timestamp\"}).set_index('name')['timestamp'].to_dict()\n",
        "\n",
        "    # Prepare timestamps data\n",
        "    timestamps['frame'] = timestamps.index\n",
        "    timestamps = timestamps.drop(columns=['section id', 'recording id'])\n",
        "\n",
        "    # Define task boundaries to filter gaze\n",
        "    tasks = ['pprep', 'pspeech', 'cprep', 'cspeech']\n",
        "    gaze_filt = pd.DataFrame()\n",
        "    k = 0\n",
        "\n",
        "    for task in tasks:\n",
        "        start_key = f\"{task}.start\"\n",
        "        end_key = f\"{task}.end\"\n",
        "\n",
        "        if start_key in events and end_key in events:\n",
        "            mask = (gaze['timestamp [ns]'] >= events[start_key]) & (gaze['timestamp [ns]'] <= events[end_key])\n",
        "            task_gaze = gaze[mask].copy()\n",
        "            task_gaze['task'] = task\n",
        "            gaze_filt = pd.concat([gaze_filt, task_gaze])\n",
        "            k +=1\n",
        "\n",
        "    if k == 0:\n",
        "        print(\"No task markers present, check data download\")\n",
        "\n",
        "    # Make sure gaze was corrected on Pupil cloud\n",
        "    if \"gaze.corrected\" not in events:\n",
        "        print(\"WARNING: No gaze correction indicator in events, check data tracker\")\n",
        "\n",
        "    # Merge gaze data with timestamps\n",
        "    gaze_filt = pd.merge_asof(gaze_filt, timestamps, on=\"timestamp [ns]\", direction='nearest')\n",
        "\n",
        "    # Create a time column\n",
        "    gaze_filt['time'] = (gaze_filt['timestamp [ns]'] - events['recording.begin']) / 1e9\n",
        "\n",
        "    # Save the result\n",
        "    out_fold = f\"[Output data directory]/{subject}\"\n",
        "    os.makedirs(out_fold, exist_ok=True)\n",
        "    output_file = os.path.join(out_fold, subject + '_gaze.csv')\n",
        "    gaze_filt.to_csv(output_file, index=False)\n",
        "\n",
        "    return gaze_filt"
      ],
      "metadata": {
        "id": "EtcVnq13XVcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function which puts a circle for each gaze point on the current frame\n",
        "def draw_gaze_points(frame, gazes, overlay_alpha=0.4):\n",
        "    \"\"\"Draw gaze points on a frame.\"\"\"\n",
        "    overlay = frame.copy()\n",
        "    for x, y in gazes:\n",
        "        coords = (int(x), int(y))\n",
        "        cv2.circle(overlay, coords, color=(0, 128, 0), radius=30, thickness=cv2.FILLED)\n",
        "        cv2.circle(overlay, coords, color=(0, 0, 255), radius=3, thickness=cv2.FILLED)\n",
        "    return cv2.addWeighted(overlay, overlay_alpha, frame, 1 - overlay_alpha, 0)"
      ],
      "metadata": {
        "id": "wCJNRLBfZuMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function which puts a circle for each gaze point on the current frame\n",
        "def draw_gaze_points(frame, gazes, overlay_alpha=0.4):\n",
        "    \"\"\"Draw gaze points on a frame.\"\"\"\n",
        "    overlay = frame.copy()\n",
        "    for x, y in gazes:\n",
        "        coords = (int(x), int(y))\n",
        "        cv2.circle(overlay, coords, color=(0, 128, 0), radius=30, thickness=cv2.FILLED)\n",
        "        cv2.circle(overlay, coords, color=(0, 0, 255), radius=3, thickness=cv2.FILLED)\n",
        "    return cv2.addWeighted(overlay, overlay_alpha, frame, 1 - overlay_alpha, 0)"
      ],
      "metadata": {
        "id": "nKGyP-7-Z6Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine if the face or person is within the \"tv screen\" bounding box\n",
        "def is_inside_bbox(center, bbox):\n",
        "    \"\"\"Check if a point (center) is inside a bounding box.\"\"\"\n",
        "    x_min, y_min, x_max, y_max = map(int, bbox.xyxy.tolist()[0])\n",
        "    return x_min < center[0] < x_max and y_min < center[1] < y_max"
      ],
      "metadata": {
        "id": "oUSW5J4AZ76u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove faces and people within our TV bounding boxes from our list of people/faces\n",
        "def filter_entities(entities, tv_bboxes, track_ids_tvs, judges):\n",
        "    \"\"\"Filter entities (faces/people) not within TV bounding boxes.\"\"\"\n",
        "    keep_indices = []\n",
        "    for entity in entities:\n",
        "        entity_center = (\n",
        "            (int(entity.xyxy.tolist()[0][0]) + int(entity.xyxy.tolist()[0][2])) // 2,\n",
        "            (int(entity.xyxy.tolist()[0][1]) + int(entity.xyxy.tolist()[0][3])) // 2\n",
        "        )\n",
        "        inside_tv = False\n",
        "        for bbox, track_id in zip(tv_bboxes, track_ids_tvs):\n",
        "            if is_inside_bbox(entity_center, bbox):\n",
        "                judges.append(track_id) # If there is a face is within the TV add this track id to the judges list\n",
        "                inside_tv = True\n",
        "                break\n",
        "        keep_indices.append(0 if inside_tv else 1)\n",
        "    return np.array(keep_indices), judges"
      ],
      "metadata": {
        "id": "Avu8vY59aGe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mask(mask, height, width):\n",
        "    \"\"\"Convert a mask to a boolean numpy array.\"\"\"\n",
        "    points = np.int32([mask.xy])\n",
        "    img_black = np.zeros((height, width, 3), np.uint8)\n",
        "    img_mask = cv2.fillPoly(img_black, points, (255, 255, 255))\n",
        "    return np.all(img_mask == [255, 255, 255], axis=2)"
      ],
      "metadata": {
        "id": "vrh5islLaHw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which determines if the gaze point touches the current segmentation mask\n",
        "def check_gaze_intersection(frame_gazes, maskbool, height, width, face_mask=None):\n",
        "    \"\"\"Check gaze intersections with masks and optionally generate visuals.\"\"\"\n",
        "    gazein = np.full(len(frame_gazes), True)\n",
        "    gazeinface = np.full(len(frame_gazes), True) if face_mask is not None else None\n",
        "\n",
        "    for k, row in enumerate(frame_gazes):\n",
        "        coords = (int(row[0]), int(row[1]))\n",
        "        circlemask = np.zeros((height, width))\n",
        "        cv2.circle(circlemask, coords, color=(1, 1, 1), radius=30, thickness=cv2.FILLED)\n",
        "        circlemask[~maskbool] = 0\n",
        "        gazein[k] = (circlemask.max() > 0)\n",
        "\n",
        "        if face_mask is not None:\n",
        "            circlemaskface = np.zeros((height, width))\n",
        "            cv2.circle(circlemaskface, coords, color=(1, 1, 1), radius=30, thickness=cv2.FILLED)\n",
        "            circlemaskface[~face_mask] = 0\n",
        "            gazeinface[k] = (circlemaskface.max() > 0)\n",
        "\n",
        "    return gazein, gazeinface"
      ],
      "metadata": {
        "id": "JYHEG6Y8aOSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which determines what to label the current gaze AOI based on gaze / mask intersections\n",
        "def label_data(gazein, gazeinface, track_id, label_opts, faceloop):\n",
        "    \"\"\"Determine the label and track_id based on gaze intersections.\"\"\"\n",
        "    if faceloop:\n",
        "        if gazeinface.all(): # If all of the gazes touch the face mask label as face\n",
        "            label_opts[1] = track_id\n",
        "        elif not gazeinface.all() and not (~gazeinface).all(): #If some in face mask and some outside label as uncodeable\n",
        "            label_opts[99] = 0\n",
        "        elif gazein.all(): # If all gaze points touch the body label as body\n",
        "            label_opts[0] = track_id\n",
        "        elif (~gazein).all(): # If all gaze points don't touch the body label as other\n",
        "            label_opts[3] = 0\n",
        "        else:\n",
        "            label_opts[99] = 0 # Otherwise, label as uncodeable\n",
        "    else:\n",
        "        if gazein.all():\n",
        "            label_opts[0] = track_id # If all gaze points touch the body label as body\n",
        "        elif (~gazein).all():\n",
        "            label_opts[3] = 0 # If all gaze points don't touch the body label as other\n",
        "        else:\n",
        "            label_opts[99] = 0 # Otherwise label as uncodeable"
      ],
      "metadata": {
        "id": "V4mS3EVxaQzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine label based on priority\n",
        "# For example if all gaze points touch the TV/Judge screen and body of the social partner\n",
        "# We label that frame as body of social partner rather than TV/Judge screen\n",
        "def finalize_label(label_opts):\n",
        "    \"\"\"Determine the final label and track based on label options.\"\"\"\n",
        "    for priority in [1, 0, 62, 3]:\n",
        "        if priority in label_opts:\n",
        "            return priority, label_opts[priority]\n",
        "    return 99, 0"
      ],
      "metadata": {
        "id": "zEGqftROc9QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main function which calls above functions\n",
        "def pcat_pipe(subject, gazes, vidgen, new):\n",
        "    # Define paths to data and model files\n",
        "    pt_path = './yolov8m-seg.pt'\n",
        "    pt_path_face = './yolov8n-face.pt'\n",
        "\n",
        "    folder = os.path.join(\n",
        "        '[data folder]',\n",
        "        subject\n",
        "    )\n",
        "    out_fold = os.path.join(\"[output folder]\", subject)\n",
        "\n",
        "    vid_path = next(glob.iglob(f\"{folder}/**/*.mp4\"), None)\n",
        "    out_file2 = os.path.join(out_fold, subject + '.avi')\n",
        "    out_hand = os.path.join(out_fold, subject + '_hand_out.avi')\n",
        "    data_file = os.path.join(out_fold, subject + '_met_data.csv')\n",
        "\n",
        "    # Save the temporarly file locally\n",
        "    out_file = os.path.join(out_fold, subject + '_met_data_vidtemp.avi')\n",
        "\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(vid_path)\n",
        "    _, image = cap.read()\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    if vidgen:\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps<29.935: # Sometimes fps is low due to error at beginning of collection before task occured\n",
        "            fps=29.94\n",
        "        print(str(fps))\n",
        "        fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
        "        out = cv2.VideoWriter(out_file, fourcc, fps, (width, height))\n",
        "        outhand = cv2.VideoWriter(out_hand, fourcc, fps, (width, height))\n",
        "\n",
        "    # Randomly select validation frames\n",
        "    tasks = gazes['task'].unique()\n",
        "    val_frames = []\n",
        "\n",
        "    for task in tasks:\n",
        "        task_frames = gazes.loc[gazes[\"task\"] == task, \"frame\"].tolist()\n",
        "        if len(set(task_frames))>500:\n",
        "            start = random.randint(task_frames[0], task_frames[-1] - int(15 * fps))\n",
        "            val_frames.extend(range(start, start + int(15 * fps) + 1))\n",
        "\n",
        "    # Prepare output data\n",
        "    outdata = gazes[['frame', 'task']].drop_duplicates()\n",
        "    outdata['Code'] = 999\n",
        "    outdata['Track_ID'] = -999\n",
        "    outdata['Validation'] = 0\n",
        "\n",
        "    all_frames = outdata['frame'].tolist()\n",
        "\n",
        "    # Get total frame count\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Load YOLO models\n",
        "    torch.cuda.set_device(0)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load primary model\n",
        "    model = YOLO(pt_path).to(device)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Using GPU\")\n",
        "\n",
        "    # Load face model\n",
        "    model_face = YOLO(pt_path_face).to(device)\n",
        "\n",
        "    # Initialize variables\n",
        "    people_ids = []\n",
        "    partner_ids = []\n",
        "    judges = []\n",
        "    non_tracker = -1\n",
        "\n",
        "    # Log start time\n",
        "    ct = datetime.datetime.now()\n",
        "    st = time.time()\n",
        "    print(f\"Start time: {ct}\")\n",
        "\n",
        "    # Loop through every frame in the video\n",
        "    for i in range(total_frames):\n",
        "        if i % 3000 == 0:\n",
        "            ct = datetime.datetime.now()\n",
        "            print(f\"time for frame number {i}\", ct)\n",
        "            outdata.to_csv(data_file, index=True)\n",
        "\n",
        "        frame_num = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # If the frame is within our task frames we process it\n",
        "        if frame_num in all_frames:\n",
        "            # Get gaze coordinates for the associated frame\n",
        "            frame_gazes = gazes[gazes.frame == frame_num].iloc[:, 3:5].values\n",
        "\n",
        "            # If this is one of our validation frames indicate it in data file and make a separate image to save to video with only gaze overlay\n",
        "            if frame_num in val_frames:\n",
        "                outdata.loc[outdata['frame'] == frame_num, ['Validation']] = 1\n",
        "                hand_frame = frame.copy()\n",
        "\n",
        "            # If there is no gaze for the current frame label as uncodeable\n",
        "            if frame_gazes.size == 0:\n",
        "                lab, track = 99, 0\n",
        "            # Otherwise run the YOLO models on our frame\n",
        "            else:\n",
        "                output = model.track(frame, verbose=False, persist=True)\n",
        "                bboxes = output[0].boxes\n",
        "                nbboxes = len(bboxes)\n",
        "\n",
        "\n",
        "                # If there is no detected things at all in the frame mark as looking at other and continue\n",
        "                if nbboxes == 0:\n",
        "                    lab, track = 3, 0\n",
        "\n",
        "                elif output[0].masks is not None:\n",
        "                    # Note 0=body, 1=face, 2=self, 3=other, 62=TV/judge, 99=uncodeable\n",
        "                    # Refine arrays to just detected people and label as self vs body\n",
        "\n",
        "                    masks = output[0].masks # Segmentation masks\n",
        "                    pred_cls = bboxes.cls.cpu().numpy() # Predicted classes of the masks\n",
        "                    pred_conf = bboxes.conf.cpu().numpy() # Predicted confidence of the object\n",
        "                    # Filter to only include people and TVs of higher confidence\n",
        "                    masks = masks[((pred_cls == 0) | (pred_cls == 62)) & (pred_conf > 0.35)]\n",
        "                    bboxes = bboxes[((pred_cls == 0) | (pred_cls == 62)) & (pred_conf > 0.35)]\n",
        "                    pred_cls= pred_cls[((pred_cls == 0) | (pred_cls == 62)) & (pred_conf > 0.35)]\n",
        "\n",
        "                    # If there is a track id (tracks same object across frames) save it\n",
        "                    # If not then create placeholders (usually when blurry image)\n",
        "                    if bboxes.id is None:\n",
        "                        # If the model didn't make track ids pick some random ones\n",
        "                        track_ids = [i for i in range(non_tracker - len(masks),non_tracker)]\n",
        "                        non_tracker = non_tracker - len(masks)\n",
        "                        track_ids = np.array(track_ids)\n",
        "\n",
        "                    else:\n",
        "                        track_ids = bboxes.id.int().cpu().numpy()\n",
        "\n",
        "                    if bboxes.id is None:\n",
        "                        non_tracker -= len(masks)\n",
        "\n",
        "                    # Label as other if no detected people or tvs\n",
        "                    if len(pred_cls) == 0:\n",
        "                        lab, track = 3, 0\n",
        "\n",
        "                    elif len(pred_cls) != 0:\n",
        "\n",
        "                         # Separate TVs and people\n",
        "                        masks_peeps, bboxes_peeps, track_ids_peeps = masks[pred_cls == 0], bboxes[pred_cls == 0], track_ids[pred_cls == 0]\n",
        "                        masks_tvs, bboxes_tvs, track_ids_tvs = masks[pred_cls == 62], bboxes[pred_cls == 62], track_ids[pred_cls == 62]\n",
        "                        track_ids_tvs = track_ids_tvs.tolist()\n",
        "\n",
        "                        # Run face detection if people are present\n",
        "                        if len(masks_peeps) > 0:\n",
        "                            output_face = model_face(frame, verbose=False)\n",
        "                            bboxes_face = output_face[0].boxes\n",
        "                            conf_face = bboxes_face.conf.cpu().numpy()\n",
        "\n",
        "                            # Filter faces based on confidence\n",
        "                            bboxes_face, conf_face = bboxes_face[conf_face > 0.35], conf_face[conf_face > 0.35]\n",
        "                        else:\n",
        "                            bboxes_face = []\n",
        "\n",
        "                        # Filter out faces within TVs and label TVs as judges\n",
        "                        if (len(bboxes_tvs) > 0) and (len(bboxes_face) > 0):\n",
        "                            keep_faces, judges = filter_entities(bboxes_face, bboxes_tvs, track_ids_tvs, judges)\n",
        "                            bboxes_face, conf_face = bboxes_face[keep_faces == 1], conf_face[keep_faces == 1]\n",
        "\n",
        "                        # Filter out people within TVs\n",
        "                        if (len(bboxes_tvs) > 0) and (len(bboxes_peeps) > 0):\n",
        "                            keep_peeps, judges = filter_entities(bboxes_peeps, bboxes_tvs, track_ids_tvs, judges)\n",
        "                            bboxes_peeps, masks_peeps, track_ids_peeps = (\n",
        "                                bboxes_peeps[keep_peeps == 1],\n",
        "                                masks_peeps[keep_peeps == 1],\n",
        "                                track_ids_peeps[keep_peeps == 1]\n",
        "                            )\n",
        "                            track_ids_peeps = track_ids_peeps.tolist()\n",
        "\n",
        "                        # Update people IDs\n",
        "                        people_ids.extend(track_ids_peeps)\n",
        "                        people_ids = list(set(people_ids))\n",
        "\n",
        "                        # If there are faces present in the frame, create a mask for them based on upper part of body mask\n",
        "                        if len(bboxes_face) > 0:\n",
        "\n",
        "                            face = True\n",
        "                            max_face = conf_face.argmax() # Get bounding box of highest confidence face (there can only be one person in room)\n",
        "                            box_face = bboxes_face[max_face]\n",
        "\n",
        "                            y_face = int(box_face.xyxy.tolist()[0][3])\n",
        "                            x_face = int((int(box_face.xyxy.tolist()[0][0]) + int(box_face.xyxy.tolist()[0][2])) / 2)\n",
        "                        else:\n",
        "                            face = False\n",
        "                            y_face = 0\n",
        "                            x_face = 0\n",
        "\n",
        "                        label_opts = {}\n",
        "\n",
        "                        # Loop through people identified in the frame\n",
        "                        for mask, bbox, track_id in zip(masks_peeps, bboxes_peeps, track_ids_peeps):\n",
        "                            maskbool = process_mask(mask, height, width)\n",
        "                            faceloop = face and maskbool[y_face-1, x_face] #If the face exists within this body\n",
        "\n",
        "                            if faceloop:\n",
        "                                face_mask = maskbool.copy()\n",
        "                                face_mask[y_face:height, :] = False\n",
        "                                maskbool[0:y_face, :] = False\n",
        "                                partner_ids.append(track_id)\n",
        "\n",
        "                                # Draw face_mask and obody mask on img\n",
        "                                if vidgen:\n",
        "                                    frame[face_mask] = frame[face_mask] * 0.5 + np.array([0, 87, 200], dtype=np.uint8) * 0.5\n",
        "                                    frame[maskbool] = frame[maskbool] * 0.5 + np.array([205, 92, 92], dtype=np.uint8) * 0.5\n",
        "                            else:\n",
        "                                face_mask = None\n",
        "                                if vidgen:\n",
        "                                    frame[maskbool] = frame[maskbool] * 0.5 + np.array([205, 92, 92],\n",
        "                                                                                       dtype=np.uint8) * 0.5\n",
        "\n",
        "                            gazein, gazeinface = check_gaze_intersection(frame_gazes, maskbool, height, width, face_mask)\n",
        "                            label_data(gazein, gazeinface, track_id, label_opts, faceloop)\n",
        "\n",
        "                        # Loop through TVs\n",
        "                        for mask, bbox, track_id in zip(masks_tvs, bboxes_tvs, track_ids_tvs):\n",
        "                            try:\n",
        "                                maskbool = process_mask(mask, height, width)\n",
        "                                if vidgen:\n",
        "                                    frame[maskbool] = frame[maskbool] * 0.5 + np.array([57, 255, 20], dtype=np.uint8) * 0.5\n",
        "                                gazein, _ = check_gaze_intersection(frame_gazes, maskbool, height, width, None)\n",
        "                                if gazein.all():\n",
        "                                    label_opts[62] = track_id\n",
        "                                elif (~gazein).all():\n",
        "                                    label_opts[3] = 0\n",
        "                                else:\n",
        "                                    label_opts[99] = 0\n",
        "                            except Exception as e: # This error occured for about 5 frames in our entire sample\n",
        "                                print(e)\n",
        "                                print(\"Error in calculating object overlay and gaze intersection for frame \" + str(frame_num))\n",
        "                                label_opts[99] = 0\n",
        "\n",
        "\n",
        "\n",
        "                        lab, track = finalize_label(label_opts)\n",
        "                else:\n",
        "                    lab = 99\n",
        "                    track = 0\n",
        "            # Save output label and track id to the dataframe\n",
        "            outdata.loc[outdata['frame'] == frame_num, ['Code']] = lab\n",
        "            outdata.loc[outdata['frame'] == frame_num, ['Track_ID']] = track\n",
        "\n",
        "            # Add gaze points and frame number to frame image and add to output video\n",
        "            if vidgen:\n",
        "                frame = draw_gaze_points(frame, frame_gazes, overlay_alpha=0.8)\n",
        "                cv2.putText(frame, str(frame_num), (985, 30), 0, 1, [0, 0, 0], thickness=3)\n",
        "                out.write(frame)\n",
        "                if frame_num in val_frames:\n",
        "                    hand_frame = draw_gaze_points(hand_frame, frame_gazes, overlay_alpha=0.8)\n",
        "                    cv2.putText(hand_frame, str(frame_num), (985, 30), 0, 1, [0, 0, 0], thickness=3)\n",
        "                    outhand.write(hand_frame)\n",
        "\n",
        "    # Check to see which people ids were never identified as having a face and save those as self (code 2)\n",
        "    partner_ids = list(set(partner_ids))\n",
        "    self_ids = list(set(people_ids) - set(partner_ids))\n",
        "    print(\"self ids:\")\n",
        "    print(self_ids)\n",
        "    print(\"partner ids:\")\n",
        "    print(partner_ids)\n",
        "    # Overwrite ids without a face as the self\n",
        "    outdata.loc[(outdata.Track_ID.isin(self_ids)) & (outdata[\"Code\"] == 0),'Code'] = 2\n",
        "    # Overwrite ids with judge as judge\n",
        "    outdata.loc[(outdata.Track_ID.isin(judges)) & (outdata[\"Code\"] == 62),'Code'] = 4\n",
        "\n",
        "    # Overwrite all tvs as judge for newer files in which there is only one monitor in the room\n",
        "    if new:\n",
        "        outdata.loc[(outdata[\"Code\"] == 62),'Code'] = 4\n",
        "    else:\n",
        "        # Tidy up judge codes based on task (cannot see other tv when not giving speech), still may need some manual checking of 62 codes to label as 3 or 4\n",
        "        if \"C\" in subject:\n",
        "            outdata.loc[(outdata[\"task\"].str.startswith('p')) & (outdata[\"Code\"] == 62), 'Code'] = 4\n",
        "        elif \"P\" in subject:\n",
        "            outdata.loc[(outdata[\"task\"].str.startswith('c')) & (outdata[\"Code\"] == 62), 'Code'] = 4\n",
        "\n",
        "    # Save output data\n",
        "    outdata.to_csv(data_file, index=False)\n",
        "    # Return time spent processing\n",
        "    et = time.time()\n",
        "    elapsed_min = (et - st) / 60\n",
        "    print('Execution time: ', elapsed_min, ' minutes')\n",
        "    # Add labels to our output video so we can get a visual of how well the model is doing\n",
        "    if vidgen:\n",
        "        out.release()\n",
        "        outhand.release()\n",
        "        # Add label to video\n",
        "        # Load Video\n",
        "        cap2 = cv2.VideoCapture(out_file)\n",
        "        height = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        width = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        fps = cap2.get(cv2.CAP_PROP_FPS)\n",
        "        fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
        "        out2 = cv2.VideoWriter(out_file2, fourcc, fps, (width, height))\n",
        "        total_frames2 = int(cap2.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        for i in range(total_frames2):\n",
        "            _, frame = cap2.read()  # Grab the frame'\n",
        "            cv2.putText(frame, str(outdata.iloc[i,2]), (30, 30), 0, 1, [0, 0, 0], thickness=3)\n",
        "            out2.write(frame)\n",
        "        out2.release()"
      ],
      "metadata": {
        "id": "gdJlvfqxc-SS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}